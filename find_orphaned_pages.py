# find_orphaned_pages.py
"""
===================================================================
Localizador de P√°ginas √ìrf√£s (find_orphaned_pages.py)
===================================================================

Objetivo:
---------
Este script analisa um site est√°tico gerado (conjunto de arquivos HTML)
para identificar "p√°ginas √≥rf√£s". Uma p√°gina √≥rf√£ √© uma p√°gina HTML
que existe no site, mas n√£o possui nenhum link interno apontando para ela
a partir de outras p√°ginas do mesmo site.

Identificar p√°ginas √≥rf√£s √© importante para SEO e para a experi√™ncia
do usu√°rio, pois essas p√°ginas podem ser dif√≠ceis de serem descobertas
por motores de busca e visitantes.

Como Usar:
-----------
O script deve ser executado a partir do terminal ap√≥s o site ter sido
completamente gerado (ex: ap√≥s rodar o build.py). Voc√™ precisa
passar o caminho para o diret√≥rio raiz do site gerado (normalmente
'public_html') como um argumento.

Sintaxe:
  python find_orphaned_pages.py <DIRETORIO_RAIZ_DO_SITE_GERADO>

Exemplo de Uso:
  python find_orphaned_pages.py public_html

Depend√™ncias:
-------------
- beautifulsoup4: Para parsear o conte√∫do HTML e extrair links.
  (Certifique-se de que est√° listado em seu requirements.txt)

O que o script faz:
-------------------
1.  Varre o diret√≥rio fornecido para encontrar todos os arquivos .html.
2.  Para cada arquivo .html encontrado, extrai todos os links <a>.
3.  Filtra e normaliza os links internos, resolvendo-os para caminhos
    relativos √† raiz do site.
4.  Compara a lista de todos os arquivos HTML existentes com a lista de
    todos os arquivos HTML que s√£o alvos de links internos.
5.  Reporta os arquivos HTML que existem mas n√£o s√£o linkados.

Nota: A p√°gina inicial (index.html na raiz) pode aparecer como √≥rf√£
se n√£o houver links expl√≠citos para ela, mas geralmente √© o ponto de
entrada do site e n√£o √© considerada uma √≥rf√£ no sentido problem√°tico.
"""
import os
import glob
from bs4 import BeautifulSoup
from urllib.parse import urlparse, unquote
import argparse

def normalize_path(path_str):
    """Normaliza um caminho, decodifica e remove barras extras."""
    return os.path.normpath(unquote(path_str)).replace(os.sep, '/')

def resolve_internal_link_target(href_value, source_file_relative_path, site_root_abs_path):
    """
    Resolve um valor href para um caminho de arquivo normalizado relativo √† raiz do site.
    Retorna None para links externos, √¢ncoras na mesma p√°gina ou links n√£o resolvidos.
    """
    parsed_href = urlparse(href_value)

    # Ignora links externos, mailto, tel, javascript, vazios ou √¢ncoras na mesma p√°gina
    if parsed_href.scheme or parsed_href.netloc or \
       href_value.startswith(('mailto:', 'tel:', 'javascript:', '#')) or \
       not href_value.strip():
        return None

    path_part = parsed_href.path

    # Constr√≥i o caminho absoluto do link alvo
    if path_part.startswith('/'):
        # Link absoluto a partir da raiz do site
        target_abs_path = os.path.join(site_root_abs_path, path_part.lstrip('/'))
    else:
        # Link relativo √† p√°gina fonte
        source_dir_abs = os.path.dirname(os.path.join(site_root_abs_path, source_file_relative_path))
        target_abs_path = os.path.join(source_dir_abs, path_part)
    
    normalized_abs_path = normalize_path(target_abs_path)

    # Se o link resolvido aponta para um diret√≥rio, assume que ele deveria apontar para o index.html dentro dele
    if os.path.isdir(normalized_abs_path):
        normalized_abs_path_with_index = normalize_path(os.path.join(normalized_abs_path, 'index.html'))
        # Verifica se o index.html realmente existe nesse diret√≥rio
        if os.path.isfile(normalized_abs_path_with_index):
             normalized_abs_path = normalized_abs_path_with_index
        # Se n√£o h√° index.html, mas o diret√≥rio existe, o link pode ser para o diret√≥rio em si.
        # No entanto, para comparar com arquivos, precisamos de um arquivo.
        # Se voc√™ n√£o quiser essa l√≥gica de index.html autom√°tico, remova este bloco.
        # else:
            # return None # Ou considera o link para diret√≥rio como n√£o apontando para um arquivo HTML espec√≠fico

    # Garante que o caminho resolvido ainda est√° dentro do diret√≥rio do site
    if not normalized_abs_path.startswith(normalize_path(site_root_abs_path)):
        return None # Link aponta para fora do site

    # Retorna o caminho relativo √† raiz do site
    relative_target_path = os.path.relpath(normalized_abs_path, site_root_abs_path)
    return normalize_path(relative_target_path)


def find_orphaned_pages_in_site(output_dir_path):
    """
    Encontra e retorna uma lista de p√°ginas HTML √≥rf√£s em um diret√≥rio de site.
    """
    site_root_abs = os.path.abspath(output_dir_path)
    all_site_html_files = set() # Armazena caminhos relativos √† raiz, ex: 'ensino/index.html'
    internally_linked_html_files = set()

    # 1. Descobre todos os arquivos HTML no site
    for html_filepath_abs in glob.glob(os.path.join(site_root_abs, '**', '*.html'), recursive=True):
        relative_path = os.path.relpath(html_filepath_abs, site_root_abs)
        all_site_html_files.add(normalize_path(relative_path))

    if not all_site_html_files:
        print(f"Nenhum arquivo HTML encontrado em '{output_dir_path}'.")
        return []

    # 2. Processa cada arquivo HTML para encontrar para onde ele linka
    for html_filepath_abs in glob.glob(os.path.join(site_root_abs, '**', '*.html'), recursive=True):
        current_file_relative = normalize_path(os.path.relpath(html_filepath_abs, site_root_abs))
        
        try:
            with open(html_filepath_abs, 'r', encoding='utf-8') as f:
                content = f.read()
            soup = BeautifulSoup(content, 'html.parser')
            
            for link_tag in soup.find_all('a', href=True):
                href = link_tag['href']
                resolved_target = resolve_internal_link_target(href, current_file_relative, site_root_abs)
                
                if resolved_target and resolved_target in all_site_html_files:
                    internally_linked_html_files.add(resolved_target)
        except Exception as e:
            print(f"‚ö†Ô∏è  Erro ao processar o arquivo '{current_file_relative}' para links: {e}")

    # 3. Identifica as p√°ginas √≥rf√£s
    # Uma p√°gina √© √≥rf√£ se ela existe (est√° em all_site_html_files)
    # mas n√£o √© o destino de nenhum link interno (n√£o est√° em internally_linked_html_files).
    orphaned_pages = all_site_html_files - internally_linked_html_files
    
    # A p√°gina inicial (index.html) geralmente n√£o √© considerada √≥rf√£,
    # mesmo que n√£o haja links para ela de outras p√°ginas, pois √© o ponto de entrada.
    # Voc√™ pode optar por remov√™-la da lista de √≥rf√£s se ela aparecer.
    homepage_normalized = normalize_path('index.html')
    if homepage_normalized in orphaned_pages:
        # print(f"Nota: '{homepage_normalized}' est√° listada como √≥rf√£, mas √© a p√°gina inicial.")
        # Opcionalmente, remova-a se n√£o quiser que seja reportada:
        # orphaned_pages.remove(homepage_normalized)
        pass


    return sorted(list(orphaned_pages))


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Encontra p√°ginas HTML √≥rf√£s (n√£o linkadas por outros links internos) em um site est√°tico gerado.",
        formatter_class=argparse.RawTextHelpFormatter
    )
    parser.add_argument(
        "directory",
        help="O diret√≥rio raiz do site gerado (ex: public_html) para verificar por p√°ginas √≥rf√£s."
    )
    args = parser.parse_args()

    if not os.path.isdir(args.directory):
        print(f"‚ùå Erro: O diret√≥rio especificado '{args.directory}' n√£o existe ou n√£o √© um diret√≥rio v√°lido.")
        exit(1)

    print(f"üîé Verificando p√°ginas √≥rf√£s em: {os.path.abspath(args.directory)}")
    orphans = find_orphaned_pages_in_site(args.directory)

    if orphans:
        print("\n‚ÄºÔ∏è --- P√ÅGINAS √ìRF√ÉS ENCONTRADAS --- ‚ÄºÔ∏è")
        print("As seguintes p√°ginas HTML existem no site, mas nenhum link interno aponta para elas:")
        for page_path in orphans:
            print(f"  - {page_path}")
        print(f"\nTotal de p√°ginas √≥rf√£s encontradas: {len(orphans)}")
        print("Considere adicionar links para estas p√°ginas a partir de conte√∫do relevante,")
        print("ou remov√™-las se n√£o forem mais necess√°rias ou se forem conte√∫do de rascunho.")
    else:
        print("\n‚úÖ Nenhuma p√°gina √≥rf√£ encontrada no site.")
    print("-" * 40 + "\n")