# find_internal_link_opportunities.py
"""
===================================================================================
Analisador de Oportunidades de Linkagem Interna (find_internal_link_opportunities.py)
===================================================================================

Objetivo:
---------
Este script analisa todos os arquivos HTML de um site est√°tico gerado para
sugerir oportunidades de linkagem interna. Para cada p√°gina do site, ele
procura outras p√°ginas cujo conte√∫do mencione termos relevantes (do t√≠tulo
ou palavras-chave da p√°gina alvo) e que ainda n√£o possuam um link para ela.

Como Usar:
-----------
Execute ap√≥s o site ter sido completamente gerado. Forne√ßa o diret√≥rio
raiz do site gerado (normalmente 'public_html') como argumento.

Sintaxe:
  python find_internal_link_opportunities.py <DIRETORIO_RAIZ_DO_SITE_GERADO>

Exemplo de Uso:
  python find_internal_link_opportunities.py public_html

Depend√™ncias:
-------------
- beautifulsoup4: Para parsear o conte√∫do HTML.
  (Certifique-se de que est√° listado em seu requirements.txt)

Importante:
-----------
- As sugest√µes s√£o baseadas em correspond√™ncia de palavras-chave e devem ser
  avaliadas manualmente quanto √† relev√¢ncia e contexto.
- A qualidade das sugest√µes depende do qu√£o bem os t√≠tulos e palavras-chave
  definem o t√≥pico de cada p√°gina e da riqueza do conte√∫do textual.
- Palavras comuns (stopwords) s√£o filtradas para melhorar a relev√¢ncia dos termos.
"""
import os
import glob
from bs4 import BeautifulSoup
from urllib.parse import urlparse, unquote
import argparse
import re

# --- Lista de Stopwords em Portugu√™s (Pode ser expandida) ---
PORTUGUESE_STOPWORDS = set([
    "de", "a", "o", "que", "e", "do", "da", "em", "um", "para", "√©", "com", "n√£o", "uma", "os", "no", "na",
    "por", "mais", "as", "dos", "como", "mas", "foi", "ao", "ele", "das", "tem", "√†", "seu", "sua", "ou",
    "ser", "quando", "muito", "h√°", "nos", "j√°", "est√°", "eu", "tamb√©m", "s√≥", "pelo", "pela", "at√©", "isso",
    "ela", "entre", "era", "depois", "sem", "mesmo", "aos", "ter", "seus", "quem", "nas", "me", "esse",
    "eles", "est√£o", "voc√™", "tinha", "foram", "essa", "num", "nem", "suas", "meu", "√†s", "minha", "t√™m",
    "numa", "pelos", "elas", "havia", "seja", "qual", "ser√°", "n√≥s", "tenho", "lhe", "deles", "essas",
    "esses", "pelas", "este", "fosse", "dele", "tu", "te", "voc√™s", "vos", "lhes", "meus", "minhas",
    "teu", "tua", "teus", "tuas", "nosso", "nossa", "nossos", "nossas", "dela", "delas", "esta", "estes",
    "estas", "aquele", "aquela", "aqueles", "aquelas", "isto", "aquilo", "estou", "est√°", "estamos",
    "est√£o", "estive", "esteve", "estivemos", "estiveram", "estava", "est√°vamos", "estavam", "estivera",
    "estiv√©ramos", "esteja", "estejamos", "estejam", "estivesse", "estiv√©ssemos", "estivessem", "estiver",
    "estivermos", "estiverem", "hei", "h√°", "havemos", "h√£o", "houve", "houvemos", "houveram", "houvera",
    "houv√©ramos", "haja", "hajamos", "hajam", "houvesse", "houv√©ssemos", "houvessem", "houver", "houvermos",
    "houverem", "houverei", "houver√°", "houveremos", "houver√£o", "houveria", "houver√≠amos", "houveriam",
    "sou", "somos", "s√£o", "era", "√©ramos", "eram", "fui", "foi", "fomos", "foram", "fora", "f√¥ramos",
    "seja", "sejamos", "sejam", "fosse", "f√¥ssemos", "fossem", "for", "formos", "forem", "serei", "ser√°",
    "seremos", "ser√£o", "seria", "ser√≠amos", "seriam", "tenho", "tem", "temos", "t√©m", "tinha", "t√≠nhamos",
    "tinham", "tive", "teve", "tivemos", "tiveram", "tivera", "tiv√©ramos", "tenha", "tenhamos", "tenham",
    "tivesse", "tiv√©ssemos", "tivessem", "tiver", "tivermos", "tiverem", "terei", "ter√°", "teremos", "ter√£o",
    "teria", "ter√≠amos", "teriam", "sobre", "qualquer", "todo", "todos", "toda", "todas", "outro", "outra",
    "outros", "outras", "tal", "tais", "mesma", "mesmos", "mesmas", "grande", "pequeno", "pouco", "muita",
    "algum", "alguma", "alguns", "algumas", "assim", "ent√£o", "logo", "porque", "pois", "onde", "quando",
    "enquanto", "sempre", "nunca", "agora", "hoje", "ontem", "amanh√£", "cedo", "tarde", "aqui", "ali",
    "l√°", "dentro", "fora", "acima", "abaixo", "frente", "atr√°s", "cada", "coisa", "caso", "cujo", "etc",
    "mim", "si", "consigo", "tipo", "ainda", "poder", "pode", "deve", "devem", "fazer", "dizer",
    "quer", "qu√™", "quem", "ser√£o", "√†quele", "√†quela", "naquele", "naquela", "neste", "nesta", "nisto",
    "nisso", "daqui", "dali", "desta", "deste", "daquele", "daquela"
])


def normalize_path(path_str):
    """Normaliza um caminho, decodifica e remove barras extras, usando / como separador."""
    return os.path.normpath(unquote(path_str)).replace(os.sep, '/')


def resolve_internal_link_target(href_value, source_file_relative_path, site_root_abs_path):
    """
    Resolve um valor href para um caminho de arquivo normalizado relativo √† raiz do site.
    Retorna None para links externos, √¢ncoras na mesma p√°gina, ou links n√£o resolvidos para um arquivo HTML interno.
    """
    parsed_href = urlparse(href_value)

    # Ignora links externos, mailto, tel, javascript, vazios ou √¢ncoras na mesma p√°gina
    if parsed_href.scheme or parsed_href.netloc or \
       href_value.startswith(('mailto:', 'tel:', 'javascript:', '#')) or \
       not href_value.strip():
        return None

    path_part = parsed_href.path

    # Constr√≥i o caminho absoluto do link alvo
    if path_part.startswith('/'):
        # Link absoluto a partir da raiz do site
        target_abs_path = os.path.join(site_root_abs_path, path_part.lstrip('/'))
    else:
        # Link relativo √† p√°gina fonte
        source_dir_abs = os.path.dirname(os.path.join(site_root_abs_path, source_file_relative_path))
        target_abs_path = os.path.join(source_dir_abs, path_part)
    
    normalized_abs_path = normalize_path(target_abs_path)

    # Se o link resolvido aponta para um diret√≥rio, tenta encontrar o index.html dentro dele
    if os.path.isdir(normalized_abs_path):
        index_in_dir_path = normalize_path(os.path.join(normalized_abs_path, 'index.html'))
        if os.path.isfile(index_in_dir_path):
             normalized_abs_path = index_in_dir_path
        else: # Se n√£o h√° index.html, o link para diret√≥rio n√£o aponta para um arquivo HTML espec√≠fico
            return None
    
    # Verifica se o caminho resolvido √© um arquivo HTML existente
    if not os.path.isfile(normalized_abs_path) or not normalized_abs_path.endswith('.html'):
        return None # N√£o √© um arquivo HTML ou n√£o existe

    # Garante que o caminho resolvido ainda est√° dentro do diret√≥rio do site
    if not normalized_abs_path.startswith(normalize_path(site_root_abs_path)):
        return None

    relative_target_path = os.path.relpath(normalized_abs_path, site_root_abs_path)
    return normalize_path(relative_target_path)


def get_site_data_and_link_map(output_dir_path):
    """
    Coleta dados (t√≠tulo, keywords, texto) e o mapa de links de sa√≠da para todas as p√°ginas HTML.
    """
    site_root_abs = os.path.abspath(output_dir_path)
    all_html_files = set()
    page_data = {}  # Formato: rel_path -> {title, keywords_set, text_content_for_matching, raw_text_content_for_snippet}
    outgoing_link_map = {} # Formato: source_rel_path -> set of target_rel_paths

    html_file_paths_abs = glob.glob(os.path.join(site_root_abs, '**', '*.html'), recursive=True)

    # Primeira passada: Coleta informa√ß√µes b√°sicas das p√°ginas
    for html_filepath_abs in html_file_paths_abs:
        relative_path = normalize_path(os.path.relpath(html_filepath_abs, site_root_abs))
        all_html_files.add(relative_path)
        try:
            with open(html_filepath_abs, 'r', encoding='utf-8') as f:
                content = f.read()
            soup = BeautifulSoup(content, 'html.parser')
            
            title_tag = soup.find('title')
            page_title = title_tag.string.strip() if title_tag and title_tag.string else os.path.basename(relative_path)
            
            meta_keywords_tag = soup.find('meta', attrs={'name': 'keywords'})
            raw_keywords_str = meta_keywords_tag['content'].strip() if meta_keywords_tag and meta_keywords_tag.get('content') else ""
            
            page_keywords_set = set(
                k.strip().lower() for k in raw_keywords_str.split(',') 
                if k.strip() and len(k.strip()) > 2 and k.strip().lower() not in PORTUGUESE_STOPWORDS
            )
            
            main_text_elements = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'span', 'article', 'section', 'td', 'th'])
            page_text_content_full = ' '.join(el.get_text(separator=' ', strip=True) for el in main_text_elements).lower()
            
            words_in_text = re.findall(r'\b[a-z√°√©√≠√≥√∫√¢√™√¥√£√µ√ß√ºA-Z√Å√â√ç√ì√ö√Ç√ä√î√É√ï√á√ú-]{3,}\b', page_text_content_full)
            filtered_text_words = [word for word in words_in_text if word not in PORTUGUESE_STOPWORDS]
            page_text_for_matching = ' '.join(filtered_text_words)

            page_data[relative_path] = {
                'title': page_title,
                'keywords_set': page_keywords_set,
                'text_content_for_matching': page_text_for_matching,
                'raw_text_content_for_snippet': page_text_content_full
            }
            outgoing_link_map[relative_path] = set() # Inicializa
        except Exception as e:
            print(f"‚ö†Ô∏è  Erro ao coletar dados de '{relative_path}': {e}")
            page_data[relative_path] = {'title': os.path.basename(relative_path), 
                                        'keywords_set': set(), 
                                        'text_content_for_matching': '',
                                        'raw_text_content_for_snippet': ''}
            outgoing_link_map[relative_path] = set()

    # Segunda passada: Constr√≥i o mapa de links de sa√≠da
    for html_filepath_abs in html_file_paths_abs:
        current_file_relative = normalize_path(os.path.relpath(html_filepath_abs, site_root_abs))
        if current_file_relative not in page_data: continue

        try:
            with open(html_filepath_abs, 'r', encoding='utf-8') as f:
                content = f.read()
            soup = BeautifulSoup(content, 'html.parser')
            
            for link_tag in soup.find_all('a', href=True):
                href = link_tag['href']
                resolved_target = resolve_internal_link_target(href, current_file_relative, site_root_abs)
                
                if resolved_target and resolved_target in all_html_files:
                    outgoing_link_map[current_file_relative].add(resolved_target)
        except Exception as e:
            print(f"‚ö†Ô∏è  Erro ao extrair links de '{current_file_relative}': {e}")
            
    return sorted(list(all_html_files)), page_data, outgoing_link_map


def suggest_internal_linking_opportunities(all_html_files, page_data, outgoing_link_map):
    """
    Analisa todas as p√°ginas e sugere oportunidades de linkagem interna.
    Retorna um dicion√°rio: target_page -> [lista de {source_page, source_title, matched_terms, context_snippet}]
    """
    all_opportunities = {} 

    for target_page_rel_path in all_html_files:
        if target_page_rel_path not in page_data: continue
        
        target_info = page_data[target_page_rel_path]
        
        target_title_words_raw = re.findall(r'\b[a-z√°√©√≠√≥√∫√¢√™√¥√£√µ√ß√ºA-Z√Å√â√ç√ì√ö√Ç√ä√î√É√ï√á√ú-]{3,}\b', target_info['title'].lower())
        target_title_words = set(word for word in target_title_words_raw if word not in PORTUGUESE_STOPWORDS)
        
        target_keywords = target_info['keywords_set'] # J√° filtrado
        
        target_topic_terms = target_title_words.union(target_keywords)
        if not target_topic_terms: continue 

        all_opportunities[target_page_rel_path] = []

        for source_page_rel_path in all_html_files:
            if source_page_rel_path == target_page_rel_path: continue 
            if source_page_rel_path not in page_data: continue

            if target_page_rel_path in outgoing_link_map.get(source_page_rel_path, set()):
                continue 

            source_info = page_data[source_page_rel_path]
            source_text_for_matching = source_info['text_content_for_matching']
            source_text_for_snippet = source_info['raw_text_content_for_snippet']
            
            found_matching_terms = set()
            for term in target_topic_terms:
                if re.search(r'\b' + re.escape(term) + r'\b', source_text_for_matching):
                    found_matching_terms.add(term)
            
            if found_matching_terms:
                context_snippet = ""
                if found_matching_terms:
                    first_found_term_for_snippet_search = list(found_matching_terms)[0]
                    try:
                        match_obj = re.search(r'\b' + re.escape(first_found_term_for_snippet_search) + r'\b', source_text_for_snippet)
                        if match_obj:
                            match_pos = match_obj.start()
                            start = max(0, match_pos - 60)
                            end = min(len(source_text_for_snippet), match_pos + len(first_found_term_for_snippet_search) + 60)
                            snippet_raw = source_text_for_snippet[start:end]
                            context_snippet = "..." + ' '.join(snippet_raw.split()) + "..."
                    except Exception:
                        context_snippet = "[N√£o foi poss√≠vel gerar snippet de contexto]"

                all_opportunities[target_page_rel_path].append({
                    'source_page': source_page_rel_path,
                    'source_page_title': source_info['title'],
                    'matching_terms_count': len(found_matching_terms),
                    'matched_terms_list': sorted(list(found_matching_terms)),
                    'context_snippet': context_snippet
                })

        if all_opportunities[target_page_rel_path]:
            all_opportunities[target_page_rel_path].sort(key=lambda x: x['matching_terms_count'], reverse=True)
            
    return all_opportunities


def print_detailed_link_opportunities_report(opportunities, page_content_data_map):
    """Imprime um relat√≥rio mais did√°tico e detalhado das oportunidades de linkagem interna."""
    
    if not any(opportunities.values()):
        print("\n‚úÖ Nenhuma oportunidade clara de nova linkagem interna foi identificada com os crit√©rios atuais.")
        print("-" * 70 + "\n")
        return

    print("\nüí° --- RELAT√ìRIO DETALHADO: OPORTUNIDADES DE LINKAGEM INTERNA --- üí°")
    print("Este relat√≥rio sugere onde voc√™ pode adicionar links internos para melhorar a conex√£o entre suas p√°ginas.")
    print("Analise cada sugest√£o para verificar se o link √© contextualmente relevante.\n")

    suggested_links_count = 0

    for target_page, suggestions_list in sorted(opportunities.items()):
        if suggestions_list: # S√≥ imprime se houver sugest√µes para esta p√°gina alvo
            target_page_info = page_content_data_map.get(target_page, {})
            target_title = target_page_info.get('title', target_page)
            # As keywords j√° s√£o um set, ent√£o convertemos para string para display
            target_keywords_str = ", ".join(sorted(list(target_page_info.get('keywords_set', [])))) if target_page_info.get('keywords_set') else "Nenhuma"


            print("=====================================================================================")
            print(f"üéØ P√ÅGINA ALVO (Para onde linkar): {target_page}")
            print(f"   T√≠tulo da P√°gina Alvo: \"{target_title}\"")
            print(f"   Palavras-chave da Alvo (filtradas): [{target_keywords_str}]")
            print("-------------------------------------------------------------------------------------")
            print("   Sugest√µes de P√ÅGINAS FONTE (De onde linkar):")
            
            for i, sug in enumerate(suggestions_list[:5], 1): # Limita a 5 sugest√µes por alvo
                suggested_links_count +=1
                print(f"\n     Sugest√£o #{i}:")
                print(f"       ‚û°Ô∏è  Link da P√°gina Fonte: {sug['source_page']}")
                print(f"          T√≠tulo da Fonte: \"{sug['source_page_title']}\"")
                print(f"          Termos Relevantes Encontrados ({sug['matching_terms_count']}): {', '.join(sug['matched_terms_list'])}")
                if sug['context_snippet']:
                    highlighted_snippet = sug['context_snippet']
                    for term in sug['matched_terms_list']:
                        try:
                            # Usa (?i) para case-insensitive no padr√£o regex, \b para palavra inteira
                            highlighted_snippet = re.sub(r'(?i)(\b' + re.escape(term) + r'\b)', 
                                                         r'**\1**', 
                                                         highlighted_snippet)
                        except re.error: 
                            pass 
                    print(f"          Contexto Sugerido (na p√°gina fonte): {highlighted_snippet}")
                else:
                    print(f"          Contexto Sugerido: [N√£o foi poss√≠vel gerar snippet de contexto]")
                print(f"          A√ß√£o: Considere adicionar um link de '{sug['source_page']}' para '{target_page}'.")
                print("          ----")
            if not suggestions_list: # Esta linha nunca ser√° alcan√ßada devido ao if externo
                print("     Nenhuma p√°gina fonte encontrada com correspond√™ncia significativa de termos.")
            print("=====================================================================================\n")

    if suggested_links_count == 0 and any(opportunities.values()): # Se houve processamento mas nenhuma sugest√£o espec√≠fica
         print("\nNenhuma sugest√£o de linkagem espec√≠fica p√¥de ser gerada com base nas palavras-chave e conte√∫do atuais, apesar de algumas p√°ginas alvo terem sido analisadas.")
    elif suggested_links_count > 0 :
        print(f"Total de sugest√µes de linkagem espec√≠ficas apresentadas: {suggested_links_count}")
    # Se `any(opportunities.values())` for falso, a primeira mensagem no topo da fun√ß√£o j√° foi impressa.
    print("-" * 70 + "\n")


# --- Main CLI Execution ---
if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Analisa todas as p√°ginas de um site e sugere oportunidades de linkagem interna.",
        formatter_class=argparse.RawTextHelpFormatter
    )
    parser.add_argument(
        "directory",
        help="O diret√≥rio raiz do site gerado (ex: public_html) para an√°lise."
    )
    args = parser.parse_args()

    if not os.path.isdir(args.directory):
        print(f"‚ùå Erro: O diret√≥rio especificado '{args.directory}' n√£o existe ou n√£o √© um diret√≥rio v√°lido.")
        exit(1)

    print(f"üîé Analisando oportunidades de linkagem interna em: {os.path.abspath(args.directory)}")
    
    all_html_files_list, page_content_data, current_outgoing_links = get_site_data_and_link_map(args.directory)
    
    if not all_html_files_list:
        print("Nenhum arquivo HTML para analisar.")
    else:
        link_opportunities = suggest_internal_linking_opportunities(
            all_html_files_list, 
            page_content_data, 
            current_outgoing_links
        )
        print_detailed_link_opportunities_report(link_opportunities, page_content_data)